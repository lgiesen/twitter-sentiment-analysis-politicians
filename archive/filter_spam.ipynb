{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from main import get_data\n",
    "(root, data_path, presidents, cities, countries, years, colors) = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_similarity(bigrams1, bigrams2):\n",
    "    \"\"\"Calculate bi-gram similarity using precomputed bigrams.\"\"\"\n",
    "    intersection = sum((bigrams1 & bigrams2).values())\n",
    "    total = sum(bigrams1.values()) + sum(bigrams2.values()) - intersection\n",
    "    return intersection / total if total else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def generate_word_bigrams(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    return [tuple(words[i:i+2]) for i in range(len(words) - 1)]  # Generate word bi-grams\n",
    "\n",
    "def calculate_bigram_similarity(text1, text2, letter_bigrams=False):\n",
    "    if letter_bigrams:\n",
    "        # Generate bigrams for each string\n",
    "        bigrams1 = [text1[i:i+2] for i in range(len(text1)-1)]\n",
    "        bigrams2 = [text2[i:i+2] for i in range(len(text2)-1)]\n",
    "    else: \n",
    "        # word bigrams\n",
    "        # Generate word bi-grams for each string\n",
    "        bigrams1 = generate_word_bigrams(text1)\n",
    "        bigrams2 = generate_word_bigrams(text2)\n",
    "\n",
    "    # Count bigrams\n",
    "    bigrams1_count = Counter(bigrams1)\n",
    "    bigrams2_count = Counter(bigrams2)\n",
    "\n",
    "    # Calculate intersection and total\n",
    "    intersection = sum((bigrams1_count & bigrams2_count).values())\n",
    "    total = sum(bigrams1_count.values()) + sum(bigrams2_count.values()) - intersection\n",
    "\n",
    "    return intersection / total if total else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Convert text to lowercase.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def filter_spam_tweets(data):\n",
    "    non_spam_data = pd.DataFrame()\n",
    "    users_to_drop = set()\n",
    "\n",
    "    for user_id, group in data.groupby('item_number'):\n",
    "        if user_id in users_to_drop:\n",
    "            continue\n",
    "\n",
    "        # Preprocess tweets to lowercase before comparison\n",
    "        tweets = [preprocess_text(text) for text in group['text'].tolist()]\n",
    "        spam_found = False\n",
    "\n",
    "        for i in range(len(tweets)):\n",
    "            for j in range(i + 1, len(tweets)):\n",
    "                similarity = calculate_bigram_similarity(tweets[i], tweets[j], letter_bigrams=False)\n",
    "                if similarity > 0.8:\n",
    "                    users_to_drop.add(user_id)\n",
    "                    spam_found = True\n",
    "                    break\n",
    "            if spam_found:\n",
    "                break\n",
    "\n",
    "        if not spam_found:\n",
    "            non_spam_data = pd.concat([non_spam_data, group], ignore_index=True)\n",
    "\n",
    "    return data[~data['item_number'].isin(users_to_drop)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exectuion manually stopped after 689m 39.8s because runtime was too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mfilter_spam_tweets\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spam_found:\n\u001b[0;32m---> 28\u001b[0m         non_spam_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnon_spam_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;241m~\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(users_to_drop)]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/concat.py:360\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03malong the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03mValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    347\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    348\u001b[0m     objs,\n\u001b[1;32m    349\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    358\u001b[0m )\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/concat.py:595\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    593\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 595\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[1;32m    599\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/concat.py:242\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    240\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/concat.py:578\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    575\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m ensure_block_shape(concat_values, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_axis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat_values\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/dtypes/concat.py:151\u001b[0m, in \u001b[0;36mconcat_compat\u001b[0;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[1;32m    148\u001b[0m             to_concat \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m to_concat]\n\u001b[1;32m    149\u001b[0m             kinds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kinds \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# GH#39817\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBehavior when concatenating bool-dtype and numeric-dtype arrays is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated; in a future version these will cast to object dtype \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    161\u001b[0m     )\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for president in presidents:\n",
    "    data = pd.read_pickle(f'{data_path}{president}.pkl')\n",
    "    data_filtered = filter_spam_tweets(data)\n",
    "    data_filtered.to_pickle(f'{data_path}{president}-filtered.pkl')\n",
    "    print(f\"{president.capitalize()}: {data.shape[0]} -> {data_filtered.shape[0]} | Filtered: {data.shape[0] - data_filtered.shape[0]} Tweets\")\n",
    "    for location in countries + cities:\n",
    "        data = pd.read_pickle(f'{data_path}{president}-{location}.pkl')\n",
    "        data_filtered = filter_spam_tweets(data)\n",
    "        data_filtered.to_pickle(f'{data_path}{president}-{location}-filtered.pkl')\n",
    "        print(f\"{president.capitalize()} - {location}: {data.shape[0]} -> {data_filtered.shape[0]} | Filtered: {data.shape[0] - data_filtered.shape[0]} Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  User Tweet Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tweet_counts(president):\n",
    "    data = pd.read_pickle(f'{data_path}{president}.pkl')\n",
    "    tweet_counts = data.groupby('user_id').size().reset_index(name='tweet_count')\n",
    "    tweet_counts.sort_values('tweet_count', ascending=False, inplace=True)\n",
    "\n",
    "    return tweet_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_user_tweet_counts = user_tweet_counts(presidents[0])\n",
    "johnson_user_tweet_counts = user_tweet_counts(presidents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_user_tweet_counts.at[trump_user_tweet_counts.shape[0]-1, 'tweet_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_number</th>\n",
       "      <th>tweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184303</th>\n",
       "      <td>226628</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_number  tweet_count\n",
       "184303       226628           11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_user_tweet_counts[trump_user_tweet_counts['tweet_count']>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_high_frequency_tweeters(tweet_counts):\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(tweet_counts['item_number'].astype(str), tweet_counts['tweet_count'], color='skyblue')\n",
    "    plt.xlabel('User ID')\n",
    "    plt.ylabel('Tweet Count')\n",
    "    plt.title('Tweet Counts per User')\n",
    "    plt.xticks(rotation=90, fontsize=8)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Frequency of Posts in a Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def users_with_high_activity(president):\n",
    "    data = pd.read_pickle(f'{data_path}{president}.pkl')\n",
    "    # Ensure 'date' is in datetime format and 'item_number' represents user ID\n",
    "    data['date'] = pd.to_datetime(data['date']).dt.date\n",
    "    high_activity_users = data.groupby(['item_number', 'date']).filter(lambda x: len(x) > 10)\n",
    "    return high_activity_users['item_number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_high_activity_users = users_with_high_activity(presidents[0])\n",
    "johnson_high_activity_users = users_with_high_activity(presidents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_activity_users_tweets(president):\n",
    "    data = pd.read_pickle(f'{data_path}{president}.pkl')\n",
    "    # Ensure 'date' is in datetime format\n",
    "    data['date'] = pd.to_datetime(data['date']).dt.date\n",
    "    # Identify users with more than 10 tweets in any single day\n",
    "    high_activity_users = data.groupby(['item_number', 'date']).filter(lambda x: len(x) > 10)['item_number'].unique()\n",
    "    # Filter out tweets from high activity users\n",
    "    return data[~data['item_number'].isin(high_activity_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trump_filtered = remove_high_activity_users_tweets(presidents[0])\n",
    "johnson_filtered = remove_high_activity_users_tweets(presidents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
